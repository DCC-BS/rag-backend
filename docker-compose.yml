services:
  rag-backend:
    build: .
    container_name: rag-backend
    ports:
      - "8080:${BACKEND_PORT}"
    environment:
      - AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
      - AZURE_TENANT_ID=${AZURE_TENANT_ID}
      - SCOPE_DESCRIPTION=${SCOPE_DESCRIPTION}
      - CLIENT_URL=${CLIENT_URL}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - LLM_URL=http://llm:8000/v1
      - RERANKER_URL=http://reranker:8000
      - EMBEDDING_URL=http://embedding:8000/v1
      - DOCLING_URL=http://docling-serve:5001
      - BACKEND_PORT=${BACKEND_PORT}
      - BACKEND_HOST=${BACKEND_HOST}
      - BACKEND_DEV=${BACKEND_DEV}
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_ENDPOINT=http://minio:9000
      - HMAC_SECRET=${HMAC_SECRET}
    develop:
      watch:
        - action: sync
          path: .
          target: /app
          ignore:
            - .venv/
            - .git/
        - action: rebuild
          path: ./pyproject.toml
        - action: restart
          path: ./.env
        - action: restart
          path: ./Dockerfile
    depends_on:
      paradedb:
        condition: service_healthy
      llm:
        condition: service_healthy
      reranker:
        condition: service_healthy
      embedding:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - rag-network

  docling-serve:
    image: quay.io/docling-project/docling-serve-cu124:latest
    container_name: docling-serve
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    ports:
      - "8004:5001"
    environment:
      - DOCLING_MODELS_PATH=/models
      - DOCLING_CONFIG_FILE=/app/docling_config.json
      - DOCLING_SERVE_ENABLE_UI=true
    volumes:
      - "${HOME}/.cache/docling:/models"
      - ./docling_config.json:/app/docling_config.json:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always
    networks:
      - rag-network

  rag-ingestion:
    build: .
    container_name: rag-ingestion
    volumes:
      - ./src:/app/src
      - ./alembic:/app/alembic
      - ./alembic.ini:/app/alembic.ini
    environment:
      - AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
      - AZURE_TENANT_ID=${AZURE_TENANT_ID}
      - SCOPE_DESCRIPTION=${SCOPE_DESCRIPTION}
      - CLIENT_URL=${CLIENT_URL}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - LLM_URL=http://llm:8000/v1
      - RERANKER_URL=http://reranker:8000
      - EMBEDDING_URL=http://embedding:8000/v1
      - DOCLING_URL=http://docling-serve:5001
      - BACKEND_PORT=${BACKEND_PORT}
      - BACKEND_HOST=${BACKEND_HOST}
      - BACKEND_DEV=${BACKEND_DEV}
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_ENDPOINT=http://minio:9000
      - HMAC_SECRET=${HMAC_SECRET}
    command: ["uv", "run", "src/rag/cli/run_ingestion.py", "--verbose"]
    depends_on:
      paradedb:
        condition: service_healthy
      embedding:
        condition: service_healthy
      minio:
        condition: service_started
      rag-backend:
        condition: service_started
      docling-serve:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - rag-network

  paradedb:
    image: paradedb/paradedb:latest
    container_name: paradedb
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - paradedb_data:/var/lib/postgresql/data/
    ports:
      - "5432:5432"
    command: "-c maintenance_work_mem=16GB"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - rag-network

  llm:
    image: vllm/vllm-openai:v0.11.0
    container_name: vllm-qwen3-32b
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["1"]
              capabilities: [gpu]
    volumes:
      - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8001:8000"
    ipc: host
    command:
      - "--port"
      - "8000"
      - "--model"
      - "Qwen/Qwen3-32B-AWQ"
      - "--max-model-len"
      - "10000"
      - "--max-num-seqs"
      - "1"
      - "--kv-cache-dtype"
      - "fp8"
      - "--reasoning-parser"
      - "qwen3"
      - "--enable-auto-tool-choice"
      - "--tool-call-parser"
      - "hermes"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--tensor-parallel-size"
      - "1"
      - "--uvicorn-log-level"
      - "warning"
      - "--disable-log-requests"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: always
    networks:
      - rag-network

  reranker:
    image: vllm/vllm-openai:v0.11.0
    container_name: vllm-reranker
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    volumes:
      - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8002:8000"
    ipc: host
    command:
      - "--port"
      - "8000"
      - "--model"
      - "Qwen/Qwen3-Reranker-0.6B"
      - "--task=score"
      - "--max-model-len=8000"
      - "--gpu-memory-utilization=0.15"
      - '--hf-overrides={"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": "true"}'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: always
    networks:
      - rag-network

  embedding:
    image: vllm/vllm-openai:v0.11.0
    container_name: vllm-embedding
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    volumes:
      - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8003:8000"
    ipc: host
    command:
      - "--port"
      - "8000"
      - "--model"
      - "Qwen/Qwen3-Embedding-0.6B"
      - "--task=embedding"
      - "--max-model-len=8000"
      - "--gpu-memory-utilization=0.15"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always
    networks:
      - rag-network

  minio:
    image: quay.io/minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    restart: always
    networks:
      - rag-network

volumes:
  paradedb_data:
  minio_data:
networks:
  rag-network:
    driver: bridge
