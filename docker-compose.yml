services:
  rag-backend:
    build: .
    container_name: rag-backend
    ports:
      - "8080:${BACKEND_PORT}"
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - LLM_URL=http://llm:8000/v1
      - RERANKER_URL=http://reranker:8000
      - EMBEDDING_URL=http://embedding:8000/v1
      - BACKEND_PORT=${BACKEND_PORT}
      - BACKEND_HOST=${BACKEND_HOST}
      - BACKEND_DEV=${BACKEND_DEV}
    develop:
      watch:
        - action: sync
          path: .
          target: /app
          ignore:
            - .venv/
            - .git/
        - action: rebuild
          path: ./pyproject.toml
        - action: restart
          path: ./.env
        - action: restart
          path: ./Dockerfile
    depends_on:
      paradedb:
        condition: service_healthy
      llm:
        condition: service_healthy
      reranker:
        condition: service_healthy
      embedding:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - rag-network

  rag-ingestion:
    build: .
    container_name: rag-ingestion
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./alembic:/app/alembic
      - ./alembic.ini:/app/alembic.ini
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [gpu]
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - LLM_URL=http://llm:8000/v1
      - RERANKER_URL=http://reranker:8000
      - EMBEDDING_URL=http://embedding:8000/v1
    command: ["uv", "run", "src/rag/cli/run_ingestion.py"]
    depends_on:
      paradedb:
        condition: service_healthy
      embedding:
        condition: service_healthy
      rag-backend:
        condition: service_started
    restart: unless-stopped
    networks:
      - rag-network

  paradedb:
    image: paradedb/paradedb:latest
    container_name: paradedb
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - paradedb_data:/var/lib/postgresql/data/
    ports:
      - "5432:5432"
    command: "-c maintenance_work_mem=16GB"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - rag-network

  llm:
    image: vllm/vllm-openai:v0.9.1
    container_name: vllm-qwen3-32b
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '1' ]
              capabilities: [gpu]
    volumes:
      - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8001:8000"
    ipc: host
    command:
      - "--port"
      - "8000"
      - "--model"
      - "Qwen/Qwen3-32B-AWQ"
      - "--max-model-len"
      - "10000"
      - "--max-num-seqs"
      - "1"
      - "--kv-cache-dtype"
      - "fp8"
      - "--enable-reasoning"
      - "--reasoning-parser"
      - "deepseek_r1"
      - "--enable-auto-tool-choice"
      - "--tool-call-parser"
      - "hermes"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--tensor-parallel-size"
      - "1"
      - "--uvicorn-log-level"
      - "warning"
      - "--disable-log-requests"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always
    networks:
      - rag-network

  reranker:
    image: vllm/vllm-openai:v0.9.1
    container_name: vllm-reranker
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    volumes:
      - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8002:8000"
    ipc: host
    command:
      - "--port"
      - "8000"
      - "--model"
      - "Qwen/Qwen3-Reranker-0.6B"
      - "--task=score"
      - "--gpu-memory-utilization=0.15"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always
    networks:
      - rag-network

  embedding:
    image: vllm/vllm-openai:v0.9.1
    container_name: vllm-embedding
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    volumes:
      - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8003:8000"
    ipc: host
    command:
      - "--port"
      - "8000"
      - "--model"
      - "Qwen/Qwen3-Embedding-0.6B"
      - "--task=embedding"
      - "--gpu-memory-utilization=0.15"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always
    networks:
      - rag-network

volumes:
  paradedb_data:

networks:
  rag-network:
    driver: bridge
