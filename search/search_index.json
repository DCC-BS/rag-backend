{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAG Application","text":"<p>A comprehensive Retrieval-Augmented Generation (RAG) application that provides intelligent document processing, embedding generation, and semantic search capabilities.</p>"},{"location":"#overview","title":"Overview","text":"<p>This RAG application combines document ingestion, vector embeddings, and intelligent retrieval to enable semantic search and question-answering over document collections. The system supports various document formats and provides role-based access control for different user groups.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Document Processing: Intelligent extraction and processing of various document formats</li> <li>Vector Embeddings: Advanced embedding generation for semantic search capabilities</li> <li>Role-Based Access Control: Directory-based access management for different user roles</li> <li>Real-time File Monitoring: Automatic detection and processing of new documents</li> <li>PostgreSQL Integration: Robust database storage with vector search capabilities</li> <li>Docker Support: Complete containerized deployment solution</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#quick-start-with-docker","title":"Quick Start with Docker","text":"<pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd rag-backend\n\n# Start all services\ndocker-compose up\n</code></pre>"},{"location":"#development-setup","title":"Development Setup","text":"<pre><code># Install dependencies\nuv install\n\n# Set up database\n# Configure your environment\n# Run the application\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#core-services","title":"Core Services","text":"<ul> <li>Document Ingestion Service - Comprehensive guide to the document ingestion pipeline, including file monitoring, processing, and embedding generation</li> </ul>"},{"location":"#configuration","title":"Configuration","text":"<p>The application uses YAML-based configuration files located in <code>src/rag/conf/</code> for different components:</p> <ul> <li><code>conf.yaml</code> - Main application configuration</li> <li><code>ingestion.yaml</code> - Document ingestion settings</li> <li><code>chat.yaml</code> - Chat interface configuration</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>The RAG application follows a modular architecture with separate services for:</p> <ul> <li>Document ingestion and processing</li> <li>Embedding generation</li> <li>Vector storage and retrieval</li> <li>API endpoints</li> <li>Role-based access management</li> </ul>"},{"location":"#support","title":"Support","text":"<p>For detailed information about specific components, refer to the individual documentation pages linked above. The codebase includes comprehensive tests and examples to help you get started quickly.</p>"},{"location":"document_api/","title":"Document Management API","text":"<p>This document describes the document management API endpoints that allow users to perform CRUD operations on documents stored in S3 buckets based on their access roles.</p>"},{"location":"document_api/#authentication","title":"Authentication","text":"<p>All endpoints require authentication via Bearer token. The authenticated user's access roles determine which documents they can access and which operations they can perform.</p>"},{"location":"document_api/#endpoints","title":"Endpoints","text":""},{"location":"document_api/#1-get-user-documents","title":"1. Get User Documents","text":"<p>GET /documents</p> <p>Returns a list of all documents accessible by the current user based on their access roles.</p> <p>Response:</p> <pre><code>{\n  \"documents\": [\n    {\n      \"id\": 1,\n      \"file_name\": \"example.pdf\",\n      \"document_path\": \"s3://bucket-role1/example.pdf\",\n      \"mime_type\": \"application/pdf\",\n      \"num_pages\": 10,\n      \"created_at\": \"2024-01-15T10:30:00Z\",\n      \"access_roles\": [\"ROLE1\"]\n    }\n  ],\n  \"total_count\": 1\n}\n</code></pre>"},{"location":"document_api/#2-get-document-by-id","title":"2. Get Document by ID","text":"<p>GET /documents/{document_id}</p> <p>Downloads the actual document file from S3 by document ID.</p> <p>Parameters: - <code>document_id</code> (path): ID of the document to download</p> <p>Response: - Binary file content with appropriate headers for file download - Returns 404 if document not found - Returns 403 if user doesn't have access to the document</p>"},{"location":"document_api/#3-upload-document","title":"3. Upload Document","text":"<p>POST /documents</p> <p>Uploads a new document to S3 bucket associated with the specified access role.</p> <p>Request Body (multipart/form-data): - <code>access_role</code> (form field): Target access role for the document - <code>file</code> (file): Document file to upload</p> <p>Response:</p> <pre><code>{\n  \"message\": \"File uploaded successfully\",\n  \"file_name\": \"example.pdf\",\n  \"additional_info\": {\n    \"bucket_name\": \"bucket-role1\",\n    \"object_key\": \"example.pdf\",\n    \"normalized_filename\": \"example.pdf\",\n    \"size\": 12345\n  }\n}\n</code></pre>"},{"location":"document_api/#4-update-document","title":"4. Update Document","text":"<p>PUT /documents/{document_id}</p> <p>Updates an existing document by uploading a new version to S3.</p> <p>Parameters: - <code>document_id</code> (path): ID of the document to update</p> <p>Request Body (multipart/form-data): - <code>access_role</code> (form field): Access role context for the operation - <code>file</code> (file): New document file content</p> <p>Response:</p> <pre><code>{\n  \"message\": \"Document updated successfully\",\n  \"document_id\": \"1\",\n  \"file_name\": \"example.pdf\",\n  \"additional_info\": {\n    \"original_filename\": \"example_updated.pdf\",\n    \"size\": 13456\n  }\n}\n</code></pre>"},{"location":"document_api/#5-delete-document","title":"5. Delete Document","text":"<p>DELETE /documents/{document_id}</p> <p>Deletes a document from both S3 and the database.</p> <p>Parameters: - <code>document_id</code> (path): ID of the document to delete</p> <p>Request Body:</p> <pre><code>{\n  \"access_role\": \"ROLE1\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"message\": \"Document deleted successfully\",\n  \"document_id\": \"1\",\n  \"file_name\": \"example.pdf\"\n}\n</code></pre>"},{"location":"document_api/#access-control","title":"Access Control","text":"<ul> <li>Users can only access documents that have at least one access role in common with their assigned roles</li> <li>For upload operations, users must have the specific access role they're uploading to</li> <li>For update/delete operations, users must have the specific access role and the document must be assigned to that role</li> <li>All operations validate user permissions before proceeding</li> </ul>"},{"location":"document_api/#file-processing","title":"File Processing","text":"<ul> <li>Uploaded files are automatically normalized (special characters, umlauts, spaces converted to underscores)</li> <li>Files are stored in S3 buckets specific to each access role (format: <code>{bucket_prefix}-{role}</code>)</li> <li>The document ingestion service will automatically process uploaded files for search indexing</li> <li>Updated documents will be reprocessed by the ingestion service when detected</li> </ul>"},{"location":"document_api/#error-responses","title":"Error Responses","text":"<ul> <li><code>400 Bad Request</code>: Invalid request parameters or missing file</li> <li><code>401 Unauthorized</code>: Authentication required or invalid token</li> <li><code>403 Forbidden</code>: Access denied to specified role or document</li> <li><code>404 Not Found</code>: Document not found</li> <li><code>500 Internal Server Error</code>: Server-side processing error</li> </ul>"},{"location":"ingestion/","title":"Document Ingestion Service","text":"<p>The document ingestion service is responsible for automatically processing documents from S3/MinIO storage, creating embeddings, and storing them in the PostgreSQL database.</p>"},{"location":"ingestion/#features","title":"Features","text":"<ul> <li>S3/MinIO Storage: Monitors S3 buckets for new documents</li> <li>Bucket-Based Access Control: Uses separate buckets for different access roles</li> <li>Document Tagging: Tracks processed documents using S3 object tags</li> <li>Document Processing: Uses Docling to extract text and create document chunks</li> <li>Embedding Generation: Creates vector embeddings for semantic search</li> <li>Database Storage: Stores documents and chunks with embeddings in PostgreSQL</li> <li>Change Monitoring: Periodic scanning for new or updated documents</li> </ul>"},{"location":"ingestion/#bucket-structure-and-access-roles","title":"Bucket Structure and Access Roles","text":"<p>The ingestion service uses bucket-based isolation for access control:</p> <pre><code>MinIO Buckets:\n\u251c\u2500\u2500 documents-el/          # Access role: \"EL\"\n\u2502   \u251c\u2500\u2500 doc1.pdf\n\u2502   \u2514\u2500\u2500 doc2.docx\n\u251c\u2500\u2500 documents-sh/          # Access role: \"SH\"\n\u2502   \u251c\u2500\u2500 doc3.pdf\n\u2502   \u2514\u2500\u2500 doc4.pptx\n\u2514\u2500\u2500 documents-el2/         # Access role: \"EL2\"\n    \u251c\u2500\u2500 doc5.pdf\n    \u2514\u2500\u2500 doc6.html\n</code></pre> <p>Each access role gets its own isolated bucket, ensuring documents are properly segmented by access permissions.</p>"},{"location":"ingestion/#document-processing-state","title":"Document Processing State","text":"<p>Documents are tracked using S3 object tags: - Unprocessed: No <code>ingestion_processed</code> tag - Processed: Has <code>ingestion_processed</code> tag with timestamp - Updated: When source document is modified after processing, it gets reprocessed</p>"},{"location":"ingestion/#supported-file-formats","title":"Supported File Formats","text":"<ul> <li>PDF (<code>.pdf</code>)</li> <li>Microsoft Word (<code>.docx</code>)</li> <li>Microsoft PowerPoint (<code>.pptx</code>)</li> <li>HTML (<code>.html</code>)</li> <li>Microsoft Excel (<code>.xlsx</code>) - optional</li> </ul>"},{"location":"ingestion/#configuration","title":"Configuration","text":"<p>The ingestion service is configured via the YAML configuration files in <code>src/rag/conf/</code>:</p> <pre><code># ingestion.yaml\nINGESTION:\n  STORAGE_TYPE: \"s3\"\n  S3_ENDPOINT: \"${oc.env:MINIO_ENDPOINT}\"\n  S3_ACCESS_KEY: \"${oc.env:MINIO_ROOT_USER}\"\n  S3_SECRET_KEY: \"${oc.env:MINIO_ROOT_PASSWORD}\"\n  BUCKET_PREFIX: \"documents\"\n  ACCESS_ROLES: [\"EL\", \"SH\", \"EL2\"]\n  WATCH_ENABLED: true\n  BATCH_SIZE: 10\n  SCAN_INTERVAL: 3600\n  PROCESSED_TAG: \"ingestion_processed\"\n</code></pre>"},{"location":"ingestion/#running-the-service","title":"Running the Service","text":""},{"location":"ingestion/#development","title":"Development","text":"<pre><code># Ensure MinIO is running with proper credentials\n# Set environment variables:\nexport MINIO_ROOT_USER=minioadmin\nexport MINIO_ROOT_PASSWORD=minioadmin\nexport MINIO_ENDPOINT=http://localhost:9000\n\n# Run the ingestion service\n./run_ingestion.sh\n</code></pre>"},{"location":"ingestion/#docker-compose","title":"Docker Compose","text":"<p>The ingestion service runs automatically as part of the Docker Compose stack:</p> <pre><code>docker-compose up rag-ingestion\n</code></pre>"},{"location":"ingestion/#service-architecture","title":"Service Architecture","text":"<pre><code>graph TD\n    A[S3/MinIO Storage] --&gt;|Scan Buckets| B[S3 Document Ingestion Service]\n    B --&gt; C[Download Objects]\n    C --&gt; D[Docling Loader]\n    D --&gt; E[Document Chunks]\n    E --&gt; F[Embedding Service]\n    F --&gt; G[PostgreSQL Database]\n    B --&gt; H[Object Tagging]\n    H --&gt; A\n\n    B --&gt; I[Initial Scan]\n    I --&gt; A\n\n    B --&gt; J[Periodic Monitoring]\n    J --&gt; A\n\n    style A fill:#E1F5FE\n    style G fill:#FFF3E0\n    style H fill:#FFF9C4\n</code></pre>"},{"location":"ingestion/#bucket-management","title":"Bucket Management","text":"<p>The service automatically: - Creates required buckets on startup if they don't exist - Names buckets using the pattern: <code>{BUCKET_PREFIX}-{access_role}</code> - Ensures proper isolation between access roles</p>"},{"location":"ingestion/#document-state-management","title":"Document State Management","text":""},{"location":"ingestion/#processing-flow","title":"Processing Flow","text":"<ol> <li>Discovery: Service scans buckets for documents</li> <li>Check Processing State: Looks for <code>ingestion_processed</code> tag</li> <li>Download: Downloads unprocessed or updated documents</li> <li>Process: Extracts text using Docling, creates embeddings</li> <li>Store: Saves to PostgreSQL database</li> <li>Tag: Marks document as processed with timestamp</li> </ol>"},{"location":"ingestion/#update-detection","title":"Update Detection","text":"<ul> <li>Compares S3 object <code>LastModified</code> time with database record creation time</li> <li>If S3 object is newer, removes processed tag and reprocesses</li> <li>Maintains data consistency between S3 and database</li> </ul>"},{"location":"ingestion/#database-schema","title":"Database Schema","text":"<p>The service works with two main database tables: - documents: Stores document metadata and access roles (document_path contains S3 URI) - document_chunks: Stores text chunks with their embeddings</p>"},{"location":"ingestion/#logging","title":"Logging","text":"<p>The service uses structured logging with JSON output for production environments. Key log events include:</p> <ul> <li>S3 connection establishment</li> <li>Bucket creation and scanning</li> <li>Document processing start/completion</li> <li>Object tagging operations</li> <li>Database operations</li> <li>Error conditions</li> </ul>"},{"location":"ingestion/#error-handling","title":"Error Handling","text":"<p>The service handles various error conditions gracefully:</p> <ul> <li>S3 connectivity issues</li> <li>Missing or corrupted objects</li> <li>Embedding service unavailability</li> <li>Database connection issues</li> <li>Unsupported file formats</li> <li>Permission errors</li> </ul> <p>Failed documents are logged but don't stop the service from processing other files.</p>"},{"location":"ingestion/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Documents are downloaded temporarily and cleaned up after processing</li> <li>Object tagging provides efficient state tracking without database queries</li> <li>Bucket-based access control provides natural isolation</li> <li>Periodic scanning interval can be adjusted based on ingestion frequency</li> <li>GPU acceleration is used when available for document processing</li> </ul>"},{"location":"ingestion/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ingestion/#common-issues","title":"Common Issues","text":"<ol> <li>Service won't start: Check S3/MinIO connectivity and credentials</li> <li>Buckets not created: Verify S3 permissions and bucket naming</li> <li>Files not being processed: Check object tagging and supported formats</li> <li>Slow processing: Check S3 network connectivity and embedding service performance</li> <li>Database errors: Verify PostgreSQL connection and table creation</li> </ol>"},{"location":"ingestion/#logs","title":"Logs","text":"<p>Check service logs for detailed error information:</p> <pre><code># Docker Compose\ndocker-compose logs rag-ingestion\n\n# Direct execution\nuv run src/rag/cli/run_ingestion.py\n</code></pre>"},{"location":"ingestion/#s3minio-management","title":"S3/MinIO Management","text":"<p>Access MinIO console at <code>http://localhost:9001</code> to: - View buckets and objects - Check object metadata and tags - Monitor storage usage - Manage access policies</p>"},{"location":"ingestion/#migration-from-file-system","title":"Migration from File System","text":"<p>To migrate from the previous file system approach:</p> <ol> <li> <p>Upload existing documents to appropriate S3 buckets:    <code>bash    # Example: Upload EL documents to documents-el bucket    aws s3 cp data/EL/ s3://documents-el/ --recursive --endpoint-url http://localhost:9000</code></p> </li> <li> <p>Clear processed tags if you want to reprocess:    <code>bash    # Remove all tags from objects (forces reprocessing)    aws s3api put-object-tagging --bucket documents-el --key document.pdf --tagging TagSet=[] --endpoint-url http://localhost:9000</code></p> </li> <li> <p>Run initial scan to process uploaded documents</p> </li> </ol>"},{"location":"orphaned_document_cleanup/","title":"Orphaned Document Cleanup","text":"<p>This document describes the orphaned document cleanup functionality that helps maintain data consistency between S3 storage and the PostgreSQL database.</p>"},{"location":"orphaned_document_cleanup/#problem-statement","title":"Problem Statement","text":"<p>When files are deleted from S3 buckets (either manually or by external processes), the corresponding document records remain in the PostgreSQL database. This creates \"orphaned\" documents - database records that reference files that no longer exist in S3 storage.</p>"},{"location":"orphaned_document_cleanup/#solution","title":"Solution","text":"<p>The enhanced document ingestion service now includes functionality to:</p> <ol> <li>Check all documents in the database against their corresponding S3 objects</li> <li>Identify orphaned documents (database records without matching S3 files)</li> <li>Remove orphaned documents from the database to maintain consistency</li> </ol>"},{"location":"orphaned_document_cleanup/#usage","title":"Usage","text":""},{"location":"orphaned_document_cleanup/#1-standalone-cleanup-script","title":"1. Standalone Cleanup Script","text":"<p>Use the dedicated CLI script for manual cleanup operations:</p> <pre><code># Run cleanup with dry-run to preview what would be deleted\nuv run src/rag/cli/cleanup_orphaned_documents.py --dry-run\n\n# Run actual cleanup\nuv run src/rag/cli/cleanup_orphaned_documents.py\n\n# Run with verbose logging\nuv run src/rag/cli/cleanup_orphaned_documents.py --verbose\n\n# Get help\nuv run src/rag/cli/cleanup_orphaned_documents.py --help\n</code></pre>"},{"location":"orphaned_document_cleanup/#2-enhanced-ingestion-service","title":"2. Enhanced Ingestion Service","text":"<p>The main ingestion service can now run cleanup during the initial scan:</p> <pre><code># Run ingestion with orphaned document cleanup\nuv run src/rag/cli/run_ingestion.py --cleanup-orphaned\n\n# Run initial scan only with cleanup\nuv run src/rag/cli/run_ingestion.py --scan-only --cleanup-orphaned\n\n# Use console logging for better readability\nuv run src/rag/cli/run_ingestion.py --cleanup-orphaned --console-logging\n</code></pre>"},{"location":"orphaned_document_cleanup/#3-programmatic-usage","title":"3. Programmatic Usage","text":"<p>You can also use the cleanup functionality programmatically:</p> <pre><code>from rag.services.document_ingestion import S3DocumentIngestionService\nfrom rag.utils.s3 import S3Utils\n\n# Initialize service\nservice = S3DocumentIngestionService()\n\n# Run standalone cleanup\ncleanup_stats = service.cleanup_orphaned_documents()\nprint(f\"Cleaned up {cleanup_stats['deleted_documents']} orphaned documents\")\n\n# Run initial scan with cleanup\nservice.initial_scan(cleanup_orphaned=True)\n\n# You can also use the S3 utility functions directly\ns3_utils = S3Utils()\n\n# Check if a specific S3 object exists\nexists = s3_utils.object_exists(\"my-bucket\", \"path/to/file.pdf\")\n\n# Parse an S3 document path\nbucket, key = s3_utils.parse_document_path(\"s3://my-bucket/path/to/file.pdf\")\n</code></pre>"},{"location":"orphaned_document_cleanup/#how-it-works","title":"How It Works","text":""},{"location":"orphaned_document_cleanup/#1-document-validation-process","title":"1. Document Validation Process","text":"<ol> <li>Query Database: Retrieve all document records from PostgreSQL</li> <li>Parse Paths: Extract bucket name and object key from each document's S3 path</li> <li>Check S3 Existence: Use S3's <code>head_object</code> API to verify file existence</li> <li>Identify Orphans: Mark documents whose S3 objects return 404 (not found)</li> <li>Clean Database: Delete orphaned document records (and their chunks via cascade)</li> </ol>"},{"location":"orphaned_document_cleanup/#2-statistics-tracking","title":"2. Statistics Tracking","text":"<p>The cleanup process tracks and reports:</p> <ul> <li><code>total_documents</code>: Total documents checked</li> <li><code>orphaned_documents</code>: Number of orphaned documents found</li> <li><code>deleted_documents</code>: Number of documents successfully deleted</li> <li><code>invalid_paths</code>: Documents with malformed S3 paths</li> <li><code>s3_check_errors</code>: Errors encountered during S3 validation</li> </ul>"},{"location":"orphaned_document_cleanup/#3-error-handling","title":"3. Error Handling","text":"<ul> <li>Invalid Paths: Documents with malformed S3 paths are logged but not deleted</li> <li>S3 Errors: Non-404 errors (permissions, connectivity) are logged and skipped</li> <li>Database Errors: Failed deletions are logged but don't stop the process</li> <li>Atomic Operations: All deletions are committed together at the end</li> </ul>"},{"location":"orphaned_document_cleanup/#safety-features","title":"Safety Features","text":""},{"location":"orphaned_document_cleanup/#dry-run-mode","title":"Dry Run Mode","text":"<p>The standalone cleanup script supports dry-run mode to preview changes:</p> <pre><code>uv run src/rag/cli/cleanup_orphaned_documents.py --dry-run\n</code></pre> <p>This will: - Check all documents against S3 - Report what would be deleted - Not make any actual changes to the database</p>"},{"location":"orphaned_document_cleanup/#detailed-logging","title":"Detailed Logging","text":"<p>All operations are logged with structured logging including: - Document IDs and file names for orphaned documents - Detailed error messages for any issues - Statistics summary at completion</p>"},{"location":"orphaned_document_cleanup/#conservative-approach","title":"Conservative Approach","text":"<p>The cleanup process is designed to be conservative: - Only deletes documents with confirmed 404 responses from S3 - Skips documents with any S3 access errors (better safe than sorry) - Logs all decisions for audit trails</p>"},{"location":"orphaned_document_cleanup/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":""},{"location":"orphaned_document_cleanup/#scheduled-cleanup","title":"Scheduled Cleanup","text":"<p>You can add cleanup to your regular maintenance schedules:</p> <pre><code># Weekly cleanup (cron example)\n0 2 * * 0 cd /path/to/project &amp;&amp; uv run src/rag/cli/cleanup_orphaned_documents.py\n</code></pre>"},{"location":"orphaned_document_cleanup/#before-ingestion","title":"Before Ingestion","text":"<p>Include cleanup in your ingestion workflows:</p> <pre><code># Clean up orphaned documents before processing new files\nuv run src/rag/cli/run_ingestion.py --scan-only --cleanup-orphaned\n</code></pre>"},{"location":"orphaned_document_cleanup/#monitoring-integration","title":"Monitoring Integration","text":"<p>The cleanup statistics can be integrated into monitoring systems:</p> <pre><code>import json\nfrom rag.services.document_ingestion import S3DocumentIngestionService\n\nservice = S3DocumentIngestionService()\nstats = service.cleanup_orphaned_documents()\n\n# Send to monitoring system\nprint(json.dumps(stats, indent=2))\n</code></pre>"},{"location":"orphaned_document_cleanup/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>S3 API Calls: Each document requires one S3 HEAD request</li> <li>Batch Processing: Database operations are batched for efficiency</li> <li>Memory Usage: All documents are loaded into memory for processing</li> <li>Network Latency: Performance depends on S3 response times</li> </ul> <p>For large databases, consider running cleanup during off-peak hours.</p>"},{"location":"orphaned_document_cleanup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"orphaned_document_cleanup/#common-issues","title":"Common Issues","text":"<ol> <li>S3 Connectivity: Ensure S3 credentials and endpoint are properly configured</li> <li>Database Permissions: Verify the service has DELETE permissions on document tables</li> <li>Large Datasets: For very large document sets, consider adding progress indicators</li> </ol>"},{"location":"orphaned_document_cleanup/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose logging for troubleshooting:</p> <pre><code>uv run src/rag/cli/cleanup_orphaned_documents.py --verbose\n</code></pre> <p>This provides detailed information about each document validation step.</p>"},{"location":"orphaned_document_cleanup/#utility-functions","title":"Utility Functions","text":"<p>The cleanup functionality is built on reusable S3 utility functions that are available in the <code>S3Utils</code> class:</p>"},{"location":"orphaned_document_cleanup/#object_existsbucket_name-object_key","title":"<code>object_exists(bucket_name, object_key)</code>","text":"<p>Checks if an object exists by performing a HEAD request.</p> <pre><code>from rag.utils.s3 import S3Utils\n\ns3_utils = S3Utils()\nexists = s3_utils.object_exists(\"documents-el\", \"reports/quarterly.pdf\")\nif exists:\n    print(\"File exists in S3\")\nelse:\n    print(\"File not found in S3\")\n</code></pre> <p>Returns: - <code>True</code> if the object exists - <code>False</code> if the object is not found (404)</p> <p>Raises: - <code>ClientError</code> for S3 errors other than 404 (permissions, connectivity, etc.)</p>"},{"location":"orphaned_document_cleanup/#parse_document_pathdocument_path","title":"<code>parse_document_path(document_path)</code>","text":"<p>Parses a document path to extract bucket name and object key.</p> <pre><code>from rag.utils.s3 import S3Utils\n\ns3_utils = S3Utils()\nresult = s3_utils.parse_document_path(\"s3://documents-el/reports/quarterly.pdf\")\nif result:\n    bucket_name, object_key = result\n    print(f\"Bucket: {bucket_name}, Key: {object_key}\")\nelse:\n    print(\"Invalid S3 path format\")\n</code></pre> <p>Input format: <code>s3://bucket-name/object-key</code></p> <p>Returns: - <code>tuple[str, str]</code> - (bucket_name, object_key) if valid - <code>None</code> if the path format is invalid</p> <p>These utilities can be used independently for other S3-related operations throughout the codebase.</p>"},{"location":"orphaned_document_cleanup/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements could include: - Progress bars for large cleanup operations - Selective cleanup by bucket or access role - Backup creation before deletion - Integration with S3 event notifications for real-time cleanup</p>"}]}